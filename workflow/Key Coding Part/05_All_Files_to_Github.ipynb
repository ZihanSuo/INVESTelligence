{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Word Count",
   "id": "a1f123b5892df04c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Define stopwords\n",
    "STOPWORDS = {\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\",\n",
    "    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\",\n",
    "    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\",\n",
    "    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\",\n",
    "    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n",
    "    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\",\n",
    "    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
    "    \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\",\n",
    "    \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
    "    \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\",\n",
    "    # Common scraping noise\n",
    "    \"said\", \"says\", \"mr\", \"mrs\", \"inc\", \"ltd\", \"corp\", \"image\", \"video\", \"market\"\n",
    "}\n",
    "\n",
    "MIN_WORD_LEN = 3\n",
    "\n",
    "def normalize_token(token: str) -> str:\n",
    "    # Remove plural 's' only if word is long enough to avoid bugs like this->thi\n",
    "    if token.endswith(\"s\") and len(token) > 4:\n",
    "        return token[:-1]\n",
    "    return token\n",
    "\n",
    "# 2. Group articles\n",
    "items = _input.all()\n",
    "groups = {}\n",
    "\n",
    "for item in items:\n",
    "    art = item.get(\"json\", {})\n",
    "    # Use query or seed as key\n",
    "    keyword = art.get(\"keyword\")\n",
    "    keyword = str(keyword).strip().lower()\n",
    "    groups.setdefault(keyword, []).append(art)\n",
    "\n",
    "# 3. Analyze\n",
    "output_items = []\n",
    "\n",
    "for keyword, articles in groups.items():\n",
    "    counter = Counter()\n",
    "\n",
    "    for art in articles:\n",
    "        # Combine title and content\n",
    "        title = art.get(\"title\") or \"\"\n",
    "        content = art.get(\"content\") or \"\"\n",
    "        text = (title + \" \" + content).lower()\n",
    "\n",
    "        if not text.strip(): continue\n",
    "\n",
    "        # Tokenize using regex\n",
    "        tokens = re.findall(r\"[a-z]+\", text)\n",
    "\n",
    "        # Filter and normalize\n",
    "        clean_tokens = []\n",
    "        for t in tokens:\n",
    "            if len(t) >= MIN_WORD_LEN and t not in STOPWORDS:\n",
    "                clean_tokens.append(normalize_token(t))\n",
    "\n",
    "        counter.update(clean_tokens)\n",
    "\n",
    "    # Output as list of lists: [[\"word\", count], ...]\n",
    "    word_count = counter.most_common(20)\n",
    "\n",
    "    output_items.append({\n",
    "        \"json\": {\n",
    "            \"keyword\": keyword,\n",
    "            \"article_count\": len(articles),\n",
    "            \"word_count\": word_count\n",
    "        }\n",
    "    })\n",
    "\n",
    "return output_items"
   ],
   "id": "794735b8e01d4db0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Scores Ranking",
   "id": "f67e7b3641f1231c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "items = _input.all()\n",
    "output_items = []\n",
    "\n",
    "for item in items:\n",
    "    group = item.get(\"json\", {})\n",
    "    keyword = group.get(\"keyword\")\n",
    "    articles = group.get(\"articles\", [])\n",
    "\n",
    "    # 1. First sort all articles by final_score\n",
    "    ranked = sorted(articles, key=lambda a: a.get(\"final_score_100\", 0), reverse=True)\n",
    "\n",
    "    # 2. Filter: Only keep articles with final_score_100 >= 50\n",
    "    ranked = [a for a in ranked if a.get(\"final_score_100\", 0) >= 50]\n",
    "\n",
    "    # 3. Then take the top 5 after filtering\n",
    "    cleaned_articles = []\n",
    "    for art in ranked[:5]:\n",
    "        cleaned_articles.append({\n",
    "            \"title\": art.get(\"title\"),\n",
    "            \"url\": art.get(\"url\"),\n",
    "            \"content\": art.get(\"content\", \"\"),\n",
    "            \"final_score\": art.get(\"final_score_100\"),\n",
    "            \"sentiment_score\": art.get(\"sentiment_polarization_score\")\n",
    "        })\n",
    "\n",
    "    output_items.append({\n",
    "        \"json\": {\n",
    "            \"keyword\": keyword,\n",
    "            \"articles\": cleaned_articles\n",
    "        }\n",
    "    })\n",
    "\n",
    "return output_items"
   ],
   "id": "ebbe8a1d5ea14132"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Entities",
   "id": "944d182f3d4bb846"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 0: Flatten articles from all blocks\n",
    "\n",
    "flat_items = []\n",
    "\n",
    "for item in _input.all():\n",
    "    block = item.get(\"json\", item)\n",
    "    articles = block.get(\"articles\", [])\n",
    "    for art in articles:\n",
    "        flat_items.append(art)\n",
    "\n",
    "# Step 1: Clean & Group by keyword\n",
    "groups = {}\n",
    "\n",
    "for data in flat_items:\n",
    "    keyword = data.get(\"keyword\")\n",
    "    entities = data.get(\"mentioned_entities\", [])\n",
    "    score = data.get(\"final_score_100\", 0)\n",
    "\n",
    "    if not keyword or not entities or score < 40:\n",
    "        continue\n",
    "\n",
    "    clean_node = {\n",
    "        \"title\": data.get(\"title\"),\n",
    "        \"url\": data.get(\"url\"),\n",
    "        \"entities\": entities,\n",
    "        \"score\": score,\n",
    "        \"sentiment\": data.get(\"sentiment_score\", 0)\n",
    "    }\n",
    "\n",
    "    k_key = str(keyword).strip().lower()\n",
    "    groups.setdefault(k_key, []).append(clean_node)\n",
    "\n",
    "# Step 2: Output formatting\n",
    "output_items = []\n",
    "\n",
    "for k, nodes in groups.items():\n",
    "    output_items.append({\n",
    "        \"json\": {\n",
    "            \"keyword\": k,\n",
    "            \"graph_data\": nodes,\n",
    "            \"node_count\": len(nodes)\n",
    "        }\n",
    "    })\n",
    "\n",
    "return output_items\n"
   ],
   "id": "b14256eb80187b06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --------------------------------------------------\n",
    "# Graph Entity Cleaner & Normalizer\n",
    "# --------------------------------------------------\n",
    "\n",
    "# 1. Configuration: Entity Normalization Map\n",
    "# Merge variations into a single canonical term\n",
    "ENTITY_MAP = {\n",
    "    \"etfs\": \"ETF\",\n",
    "    \"crypto\": \"Cryptocurrency\",\n",
    "    \"cryptocurrencies\": \"Cryptocurrency\",\n",
    "    \"crypto market\": \"Cryptocurrency\",\n",
    "    \"federal reserve\": \"Fed\",\n",
    "    \"us military\": \"US Government\",\n",
    "    \"sec\": \"SEC\",\n",
    "    \"securities and exchange commission\": \"SEC\",\n",
    "    \"ai\": \"Artificial Intelligence\",\n",
    "    \"stock market\": \"Equities\",\n",
    "    \"options market\": \"Derivatives\",\n",
    "    \"blackrock\": \"BlackRock\",  # Ensure casing\n",
    "}\n",
    "\n",
    "# 2. Configuration: Generic Stopwords to Remove\n",
    "# These words are too broad to form meaningful clusters\n",
    "STOPWORDS = {\n",
    "    \"report\", \"analysts\", \"today\", \"price\", \"market\", \"sector\", \"industry\",\n",
    "    \"investors\", \"growth\", \"demand\", \"supply\", \"news\", \"company\"\n",
    "}\n",
    "\n",
    "items = _input.all()\n",
    "output_items = []\n",
    "\n",
    "for item in items:\n",
    "    group = item.get(\"json\", {})\n",
    "    keyword = str(group.get(\"keyword\", \"\")).lower().strip()\n",
    "\n",
    "    # Add the main keyword itself to stopwords for this group\n",
    "    # (e.g., remove \"Bitcoin\" node from \"Bitcoin\" keyword graph)\n",
    "    current_stopwords = STOPWORDS.copy()\n",
    "    current_stopwords.add(keyword)\n",
    "\n",
    "    cleaned_articles = []\n",
    "\n",
    "    for article in group.get(\"graph_data\", []):\n",
    "        original_entities = article.get(\"entities\", [])\n",
    "        clean_entities = set()\n",
    "\n",
    "        for ent in original_entities:\n",
    "            # Normalize: Lowercase for comparison\n",
    "            ent_lower = str(ent).lower().strip()\n",
    "\n",
    "            # Check 1: Is it the keyword itself?\n",
    "            if ent_lower == keyword:\n",
    "                continue\n",
    "\n",
    "            # Check 2: Is it a generic stopword?\n",
    "            if ent_lower in current_stopwords:\n",
    "                continue\n",
    "\n",
    "            # Check 3: Map to canonical name (e.g., \"ETFs\" -> \"ETF\")\n",
    "            # Uses the map, otherwise Title Cases the original\n",
    "            canonical = ENTITY_MAP.get(ent_lower)\n",
    "            if not canonical:\n",
    "                # Default formatting: Title Case (e.g., \"ai stocks\" -> \"Ai Stocks\")\n",
    "                # Better: Use the original casing if available, or .title()\n",
    "                canonical = ent\n",
    "\n",
    "            clean_entities.add(canonical)\n",
    "\n",
    "        # Update the article with cleaned entities\n",
    "        # Convert set back to list\n",
    "        article[\"entities\"] = list(clean_entities)\n",
    "\n",
    "        # Only keep articles that still have entities left\n",
    "        if article[\"entities\"]:\n",
    "            cleaned_articles.append(article)\n",
    "\n",
    "    # Update group\n",
    "    group[\"graph_data\"] = cleaned_articles\n",
    "    group[\"node_count\"] = len(cleaned_articles)\n",
    "\n",
    "    output_items.append({\"json\": group})\n",
    "\n",
    "return output_items"
   ],
   "id": "a155dfc832f6b0b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Alpha Quadrant",
   "id": "b4a6d9f4e87deaad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "items = _input.all()\n",
    "cleaned_rows = []\n",
    "\n",
    "for item in items:\n",
    "    # 1. Get the group data\n",
    "    group = item.get(\"json\", {})\n",
    "\n",
    "    # 2. Get the list of articles from this group\n",
    "    articles = group.get(\"articles\", [])\n",
    "\n",
    "    # 3. Iterate through articles to extract specific fields\n",
    "    for art in articles:\n",
    "        # Validation: Skip if essential plotting axes are missing\n",
    "        if art.get(\"source_credibility\") is None or art.get(\"materiality_score\") is None:\n",
    "            continue\n",
    "\n",
    "        # 4. Create the clean object (Selecting only needed columns)\n",
    "        # Flattening the structure: One dict per article\n",
    "        row = {\n",
    "            \"keyword\": art.get(\"keyword\") or group.get(\"keyword\"),\n",
    "            \"title\": art.get(\"title\"),\n",
    "            \"url\": art.get(\"url\"),\n",
    "            \"source_credibility\": float(art.get(\"source_credibility\")),\n",
    "            \"materiality_score\": float(art.get(\"materiality_score\")),\n",
    "            \"pickup_count\": int(art.get(\"pickup_count\") or 0),\n",
    "            \"sentiment_score\": float(art.get(\"sentiment_score\") or 0),\n",
    "            \"final_score\": float(art.get(\"final_score_100\") or 0)\n",
    "        }\n",
    "\n",
    "        cleaned_rows.append({\"json\": row})\n",
    "\n",
    "return cleaned_rows"
   ],
   "id": "332710363896d03b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Sentiment Statistics",
   "id": "1f14b7c63dda954a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "items = _input.all()\n",
    "output_items = []\n",
    "\n",
    "import math\n",
    "\n",
    "for it in items:\n",
    "    group = it.get(\"json\", {})\n",
    "    keyword = group.get(\"keyword\")\n",
    "    articles = group.get(\"articles\", [])\n",
    "\n",
    "    sentiments = []\n",
    "    weights = []\n",
    "\n",
    "    for art in articles:\n",
    "        s = art.get(\"sentiment_score\")\n",
    "        if s is None:\n",
    "            continue\n",
    "        sentiments.append(float(s))\n",
    "        weights.append(float(art.get(\"final_score\", 0)) or 0.0)\n",
    "\n",
    "    n = len(sentiments)\n",
    "\n",
    "    if n == 0:\n",
    "        summary = {\n",
    "            \"keyword\": keyword,\n",
    "            \"n_articles\": 0\n",
    "        }\n",
    "    else:\n",
    "        sentiments_sorted = sorted(sentiments)\n",
    "        mean_senti = sum(sentiments) / n\n",
    "        median_senti = sentiments_sorted[n // 2] if n % 2 == 1 else \\\n",
    "            0.5 * (sentiments_sorted[n // 2 - 1] + sentiments_sorted[n // 2])\n",
    "\n",
    "        var = sum((x - mean_senti) ** 2 for x in sentiments) / n\n",
    "        std_senti = math.sqrt(var)\n",
    "\n",
    "        total_weight = sum(weights)\n",
    "        if total_weight > 0:\n",
    "            weighted_mean_senti = sum(w * s for w, s in zip(weights, sentiments)) / total_weight\n",
    "        else:\n",
    "            weighted_mean_senti = mean_senti\n",
    "\n",
    "        def ratio(cond):\n",
    "            cnt = sum(1 for x in sentiments if cond(x))\n",
    "            return cnt / n\n",
    "\n",
    "        pct_strong_neg = ratio(lambda x: x < -0.5)\n",
    "        pct_weak_neg   = ratio(lambda x: -0.5 <= x < -0.2)\n",
    "        pct_neutral    = ratio(lambda x: -0.2 <= x <= 0.2)\n",
    "        pct_weak_pos   = ratio(lambda x: 0.2 < x <= 0.5)\n",
    "        pct_strong_pos = ratio(lambda x: x > 0.5)\n",
    "\n",
    "        summary = {\n",
    "          \"keyword\": keyword,\n",
    "          \"n_articles\": n,\n",
    "          \"mean_senti\": round(mean_senti, 4),\n",
    "          \"median_senti\": round(median_senti, 4),\n",
    "          \"std_senti\": round(std_senti, 4),\n",
    "          \"weighted_mean_senti\": round(weighted_mean_senti, 4),\n",
    "\n",
    "          \"pct_strong_neg\": f\"{pct_strong_neg * 100:.2f}%\",\n",
    "          \"pct_weak_neg\": f\"{pct_weak_neg * 100:.2f}%\",\n",
    "          \"pct_neutral\": f\"{pct_neutral * 100:.2f}%\",\n",
    "          \"pct_weak_pos\": f\"{pct_weak_pos * 100:.2f}%\",\n",
    "          \"pct_strong_pos\": f\"{pct_strong_pos * 100:.2f}%\"\n",
    "        }\n",
    "\n",
    "    output_items.append({\"json\": summary})\n",
    "\n",
    "return output_items"
   ],
   "id": "3f2440de352604aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import statistics\n",
    "\n",
    "results = []\n",
    "\n",
    "for item in items:\n",
    "    data = item.get(\"json\", item)\n",
    "    keyword = data.get(\"keyword\")\n",
    "    articles = data.get(\"articles\", [])\n",
    "\n",
    "    if not articles:\n",
    "        results.append({\n",
    "            \"json\": {\n",
    "                \"keyword\": keyword,\n",
    "                \"top_positive_article\": None,\n",
    "                \"top_negative_article\": None,\n",
    "                \"range_senti\": None,\n",
    "                \"range_category\": None,\n",
    "                \"range_summary\": None\n",
    "            }\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Collect valid float scores\n",
    "    sentiments = []\n",
    "    for a in articles:\n",
    "        val = a.get(\"sentiment_score\")\n",
    "        if val is None:\n",
    "            val = a.get(\"sentiment_score\")\n",
    "        \n",
    "        if val is not None:\n",
    "            sentiments.append(float(val))\n",
    "    \n",
    "    # Calculate Median for this group (Default to 0.0 if empty)\n",
    "    if not sentiments:\n",
    "        sentiments = [0.0]\n",
    "        senti_median = 0.0\n",
    "    else:\n",
    "        senti_median = statistics.median(sentiments)\n",
    "\n",
    "    # 1. Range calculation\n",
    "    range_senti = max(sentiments) - min(sentiments)\n",
    "    range_senti = round(range_senti, 4)\n",
    "\n",
    "    if range_senti <= 0.3:\n",
    "        range_category = \"coherent\"\n",
    "        range_summary = \"Sentiment is highly aligned across articles.\"\n",
    "    elif range_senti <= 0.8:\n",
    "        range_category = \"divergent\"\n",
    "        range_summary = \"Sentiment shows moderate divergence.\"\n",
    "    else:\n",
    "        range_category = \"polarized\"\n",
    "        range_summary = \"Sentiment is sharply polarized.\"\n",
    "\n",
    "    # Helper: Get score safely, using Median for None values\n",
    "    def get_senti_score(x):\n",
    "        val = x.get(\"sentiment_score\")\n",
    "        if val is None:\n",
    "            val = x.get(\"sentiment_score\")\n",
    "        \n",
    "        # Use median if still None\n",
    "        if val is None:\n",
    "            return senti_median\n",
    "        return float(val)\n",
    "\n",
    "    # 2. Top Positive (Using Median fallback)\n",
    "    pos = max(articles, key=get_senti_score)\n",
    "    \n",
    "    top_positive_clean = {\n",
    "        \"title\": pos.get(\"title\"),\n",
    "        \"url\": pos.get(\"url\"),\n",
    "        \"content\": pos.get(\"content\"),\n",
    "        \"sentiment_score\": pos.get(\"sentiment_score\"),\n",
    "        \"final_score\": pos.get(\"final_score\") if pos.get(\"final_score\") is not None else pos.get(\"final_score_100\"),\n",
    "    }\n",
    "\n",
    "    # 3. Top Negative (Using Median fallback)\n",
    "    neg = min(articles, key=get_senti_score)\n",
    "    \n",
    "    top_negative_clean = {\n",
    "        \"title\": neg.get(\"title\"),\n",
    "        \"url\": neg.get(\"url\"),\n",
    "        \"content\": neg.get(\"content\") or neg.get(\"snippet\"),\n",
    "        \"sentiment_score\": neg.get(\"sentiment_score\"),\n",
    "        \"final_score\": neg.get(\"final_score\") if neg.get(\"final_score\") is not None else neg.get(\"final_score_100\"),\n",
    "    }\n",
    "\n",
    "    results.append({\n",
    "        \"json\": {\n",
    "            \"keyword\": keyword,\n",
    "            \"top_positive_article\": top_positive_clean,\n",
    "            \"top_negative_article\": top_negative_clean,\n",
    "            \"range_senti\": range_senti,\n",
    "            \"range_category\": range_category,\n",
    "            \"range_summary\": range_summary\n",
    "        }\n",
    "    })\n",
    "\n",
    "return results"
   ],
   "id": "c2f120f018e5b7f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
